{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qJpdKyRQ5vnK",
    "outputId": "979f2dbe-b129-43b3-a4ba-c59fccc859aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Downloading PyMuPDF-1.18.6-cp37-cp37m-win_amd64.whl (5.3 MB)\n",
      "Installing collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.18.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "juEXHHVjBa4k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting resume-parser\n",
      "  Downloading resume_parser-0.6-py3-none-any.whl (4.9 MB)\n",
      "Collecting numpy>=1.19.1\n",
      "  Downloading numpy-1.20.0-cp37-cp37m-win_amd64.whl (13.6 MB)\n",
      "Requirement already satisfied: docx2txt>=0.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from resume-parser) (0.8)\n",
      "Processing c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\45\\6c\\46\\a1865e7ba706b3817f5d1b2ff7ce8996aabdd0d03d47ba0266\\nltk-3.5-py3-none-any.whl\n",
      "Collecting stemming>=1.0.1\n",
      "  Downloading stemming-1.0.1.tar.gz (10 kB)\n",
      "Collecting pdfplumber>=0.5.23\n",
      "  Downloading pdfplumber-0.5.25.tar.gz (42 kB)\n",
      "Collecting spacy>=2.3.2\n",
      "  Downloading spacy-3.0.0-cp37-cp37m-win_amd64.whl (11.6 MB)\n",
      "Collecting pdfminer.six>=20200517\n",
      "  Downloading pdfminer.six-20201018-py3-none-any.whl (5.6 MB)\n",
      "Collecting phonenumbers>=8.12.7\n",
      "  Downloading phonenumbers-8.12.17-py2.py3-none-any.whl (2.6 MB)\n",
      "Collecting tika>=1.24\n",
      "  Downloading tika-1.24.tar.gz (28 kB)\n",
      "Requirement already satisfied: pandas>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from resume-parser) (1.2.0)\n",
      "Collecting regex\n",
      "  Downloading regex-2020.11.13-cp37-cp37m-win_amd64.whl (269 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk>=3.5->resume-parser) (0.14.1)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk>=3.5->resume-parser) (7.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk>=3.5->resume-parser) (4.55.0)\n",
      "Requirement already satisfied: Pillow>=7.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pdfplumber>=0.5.23->resume-parser) (7.0.0)\n",
      "Collecting Wand\n",
      "  Downloading Wand-0.6.5-py2.py3-none-any.whl (138 kB)\n",
      "Collecting typer<0.4.0,>=0.3.0\n",
      "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy>=2.3.2->resume-parser) (2.11.1)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.4-cp37-cp37m-win_amd64.whl (6.5 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy>=2.3.2->resume-parser) (20.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy>=2.3.2->resume-parser) (2.25.1)\n",
      "Collecting thinc<8.1.0,>=8.0.0\n",
      "  Downloading thinc-8.0.1-cp37-cp37m-win_amd64.whl (1.0 MB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.5-cp37-cp37m-win_amd64.whl (108 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.0\n",
      "  Downloading srsly-2.4.0-cp37-cp37m-win_amd64.whl (449 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy>=2.3.2->resume-parser) (45.2.0.post20200210)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.0\n",
      "  Downloading spacy_legacy-3.0.1-py2.py3-none-any.whl (7.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.5-cp37-cp37m-win_amd64.whl (35 kB)\n",
      "Collecting pathy\n",
      "  Downloading pathy-0.3.4-py3-none-any.whl (33 kB)\n",
      "Collecting pydantic<1.8.0,>=1.7.1\n",
      "  Downloading pydantic-1.7.3-cp37-cp37m-win_amd64.whl (1.7 MB)\n",
      "Collecting wasabi<1.1.0,>=0.8.1\n",
      "  Downloading wasabi-0.8.2-py3-none-any.whl (23 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.1\n",
      "  Downloading catalogue-2.0.1-py3-none-any.whl (9.6 kB)\n",
      "Collecting typing-extensions>=3.7.4; python_version < \"3.8\"\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.5-cp37-cp37m-win_amd64.whl (20 kB)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy>=2.3.2->resume-parser) (1.5.0)\n",
      "Requirement already satisfied: chardet; python_version > \"3.0\" in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pdfminer.six>=20200517->resume-parser) (3.0.4)\n",
      "Requirement already satisfied: cryptography in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pdfminer.six>=20200517->resume-parser) (2.8)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pdfminer.six>=20200517->resume-parser) (2.1.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas>=1.1.0->resume-parser) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas>=1.1.0->resume-parser) (2.8.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->spacy>=2.3.2->resume-parser) (1.1.1)\n",
      "Requirement already satisfied: six in c:\\users\\hp\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy>=2.3.2->resume-parser) (1.12.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy>=2.3.2->resume-parser) (2.4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.2->resume-parser) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.2->resume-parser) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.2->resume-parser) (1.25.8)\n",
      "Collecting smart-open<4.0.0,>=2.2.0\n",
      "  Downloading smart_open-3.0.0.tar.gz (113 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy>=2.3.2->resume-parser) (2.2.0)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from cryptography->pdfminer.six>=20200517->resume-parser) (1.14.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hp\\anaconda3\\lib\\site-packages (from cffi!=1.11.3,>=1.8->cryptography->pdfminer.six>=20200517->resume-parser) (2.19)\n",
      "Building wheels for collected packages: stemming, pdfplumber, tika, smart-open\n",
      "  Building wheel for stemming (setup.py): started\n",
      "  Building wheel for stemming (setup.py): finished with status 'done'\n",
      "  Created wheel for stemming: filename=stemming-1.0.1-py3-none-any.whl size=11144 sha256=bee96da5825d32abde9d0827c7b0e1df41ef58ef5146a628724ca5f0e69d6d28\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\33\\cc\\e7\\4ad5c9017df40987db1b663d75c3e77667bbaa996475669611\n",
      "  Building wheel for pdfplumber (setup.py): started\n",
      "  Building wheel for pdfplumber (setup.py): finished with status 'done'\n",
      "  Created wheel for pdfplumber: filename=pdfplumber-0.5.25-py3-none-any.whl size=31563 sha256=4949c715e4d58eae185b147ec308ff2b0cce17d8b70f55db564f5629dd0180b1\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\8c\\65\\d1\\67ceda23cc7fd669025b485b7f93bea3ee5799a09c95e74452\n",
      "  Building wheel for tika (setup.py): started\n",
      "  Building wheel for tika (setup.py): finished with status 'done'\n",
      "  Created wheel for tika: filename=tika-1.24-py3-none-any.whl size=32888 sha256=534d26b3ebfedc4bc4315f5acb5325ed31d78f3e7e5f52368876eff4ed6232b2\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\ec\\2b\\38\\58ff05467a742e32f67f5d0de048fa046e764e2fbb25ac93f3\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-3.0.0-py3-none-any.whl size=107102 sha256=c161e76c07fbaf66bf3ff19cd3833a03411c8af2e1ac1ef292698636d7f52f84\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\83\\a6\\12\\bf3c1a667bde4251be5b7a3368b2d604c9af2105b5c1cb1870\n",
      "Successfully built stemming pdfplumber tika smart-open\n",
      "Installing collected packages: numpy, regex, nltk, stemming, pdfminer.six, Wand, pdfplumber, typer, blis, pydantic, murmurhash, catalogue, srsly, typing-extensions, cymem, preshed, wasabi, thinc, spacy-legacy, smart-open, pathy, spacy, phonenumbers, tika, resume-parser\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.18.1\n",
      "    Uninstalling numpy-1.18.1:\n",
      "      Successfully uninstalled numpy-1.18.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: textract 1.6.3 has requirement pdfminer.six==20181108, but you'll have pdfminer-six 20201018 which is incompatible.\n",
      "ERROR: tensorflow 2.3.1 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.20.0 which is incompatible.\n",
      "ERROR: pdfplumber 0.5.25 has requirement pdfminer.six==20200517, but you'll have pdfminer-six 20201018 which is incompatible.\n",
      "ERROR: typer 0.3.2 has requirement click<7.2.0,>=7.1.1, but you'll have click 7.0 which is incompatible.\n",
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\\\users\\\\hp\\\\anaconda3\\\\lib\\\\site-packages\\\\~umpy\\\\core\\\\_multiarray_tests.cp37-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install resume-parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aw5p5wNoIv1E",
    "outputId": "9c8e6cc8-6683-450e-a3e5-3b0b12eb52c9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 137,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "C_G_5Tlg6RHD"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from resume_parser import resumeparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os,glob\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5LjI7oUs6EcH"
   },
   "outputs": [],
   "source": [
    "mkdir Resume_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VoTmsceh6Nhe"
   },
   "outputs": [],
   "source": [
    "mkdir Resume_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOfADwJF7jOv"
   },
   "source": [
    "**PDF TO TEXT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_zruSsSI6VGi",
    "outputId": "044201ef-62c5-40b9-edb2-13b89bfabb47"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mupdf: cannot recognize version marker\n",
      "mupdf: cannot find startxref\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import os\n",
    "Txt_dir=\"/content/Resume_txt\"\n",
    "i=1\n",
    "\n",
    "for subdir, dirs, files in os.walk('/content/Resume_pdf'):\n",
    "    for f in files:\n",
    "      \n",
    "      PDFfilepath = subdir + os.sep + f\n",
    "      file=fitz.open(PDFfilepath)\n",
    "      for pageNumber,page in enumerate(file.pages(),start=1):\n",
    "            file_name=f.replace(\".pdf\",\".txt\");\n",
    "            TXTfilepath=Txt_dir+os.sep+file_name\n",
    "\n",
    "            text=page.getText()\n",
    "            txt=open(TXTfilepath,\"a\")\n",
    "            txt.writelines(text)\n",
    "            txt.close()\n",
    "    \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "49JttsW56YgQ"
   },
   "outputs": [],
   "source": [
    "Entity={}\n",
    "Entity[\"email\"]=\"([a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+)\"\n",
    "Entity[\"contact\"]='(?i)contact|(?i)phone|[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]'\n",
    "Entity[\"linkedin\"]=\"(?!)linkedin\\s(?!\\w+)|linkedin.com/[^ ]+\"\n",
    "Entity[\"github\"]=\"(?i)github\\s(?!\\w+)\"\n",
    "Entity[\"address\"]=\"(?i)address\\s(?!\\w+)|(?i)location\\s(?!\\w+)\"\n",
    "Entity[\"objective\"]=\"(?i)objective\\s(?!\\w+)|(?i)SUMMARY\\s(?!\\w+)|(?i)objective\\W|(?i)SUMMARY\\W\"\n",
    "Entity[\"Education\"]=\"(?i)EDUCATION\\s(?!\\w+)|(?i)EDUCATION:\"\n",
    "Entity[\"skills\"]=\"(?i)skills\\s(?!\\w+)|(?i)skills\\s(?!\\w+)|(?i)skill\\sset\"\n",
    "Entity[\"Tech skills\"]=\"(?i)tech\\w+\\sskills\\s(?!\\w+)|(?i)tech\\w+\\sskills\\W\"\n",
    "Entity[\"Projetcs\"]=\"(?i)projects\\s(?!\\w+)|(?i)PERSONAL PROJECTS\\s(?!\\w+)|(?i)Projects\\W\"\n",
    "Entity[\"Internship\"]=\"(?i)internship\\s(?!\\w+)|(?i)internships\\s(?!\\w+)\"\n",
    "Entity[\"Trainings\"]=\"(?i)TRAININGS\\s(?!\\w+)\"\n",
    "Entity[\"Work_experience\"]=\"(?i)EXPERIENCE\\s(?!\\w+)|(?i)Experience:\"\n",
    "Entity[\"Company\"]=\"(?i)ORGANIZATIONS\\s(?!\\w+)\"\n",
    "Entity[\"Awards\"]=\"(?i)achivement\\s(?!\\w+)|(?i)awards[^\\S]|(?i)CERTIFICATES\\s(?!\\w+)|(?i)CERTIFICATIONS\\W\"\n",
    "Entity[\"Interest\"]=\"(?i)interests\\s(?!\\w+)|(?i)HOBBIES\\s(?!\\w+)\"\n",
    "Entity[\"Language\"]=\"(?i)languages(?!\\w+)\"\n",
    "Entity[\"Work_sample\"]=\"(?i)WORK\\sSAMPLES\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMIQLNvDMyC3"
   },
   "source": [
    "**Matching**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNJKn4d8IeTL"
   },
   "source": [
    "\n",
    "\n",
    "> FINAL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "qANpwrh_0tmn"
   },
   "outputs": [],
   "source": [
    "file_name=[]\n",
    "class Resume:\n",
    "  def __init__(self,filedir,job):\n",
    "    self.filedir = filedir\n",
    "    self.job=job\n",
    "\n",
    " \n",
    "  def resume_parser(self):\n",
    "    final_database=[]\n",
    "    for filename in glob.glob(os.path.join(self.filedir, '*.txt')):\n",
    "      with open(filename, 'r') as f:\n",
    "          file_name.append(filename.replace('/content/Resume_txt/','')) \n",
    "        \n",
    "          lines=f.readlines()\n",
    "        \n",
    "\n",
    "          seen = set()\n",
    "          Heading=[]\n",
    "          pattern=[]\n",
    "\n",
    "          #Entities\n",
    "          for line in lines:\n",
    "          \n",
    "          \n",
    "            for key, value in Entity.items():\n",
    "              if re.findall(value,str(line)):\n",
    "                if key not in seen:\n",
    "                  pattern.append(re.findall(value,line))\n",
    "                  Heading.append(key)\n",
    "                  seen.add(key)\n",
    "          \n",
    "          #pattern_match\n",
    "          pattern_list=[]\n",
    "          for item in pattern:\n",
    "            pattern_list.append(item[-1].strip())\n",
    "\n",
    "        \n",
    "\n",
    "          #insert info at index 0\n",
    "          Heading.insert(0,\"info\")\n",
    "          pattern_list.insert(0,\"Info-\")\n",
    "\n",
    "          #split\n",
    "          f=open(f.name,'r')\n",
    "          content=f.read().replace(\"\\n\",\" \")\n",
    "          \n",
    "          #Escape character\n",
    "          match=[]\n",
    "          for word in pattern_list:\n",
    "            match.append(re.escape(word))\n",
    "\n",
    "          split=re.split(\"|\".join(match),content)\n",
    "\n",
    "          split=list(zip(pattern_list,split))\n",
    "\n",
    "          for i in range(len(split)):\n",
    "            split[i]=' '.join(split[i])\n",
    "\n",
    "\n",
    "\n",
    "          #print frame\n",
    "\n",
    "          pd.set_option('display.max_colwidth', -1)\n",
    "          final_database.append(pd.DataFrame(\n",
    "            {'Heading': Heading ,\n",
    "            'Content': split}))\n",
    "\n",
    "          #extract important info\n",
    "    info=[]\n",
    "    for i in range(len(final_database)):\n",
    "      Entities=final_database[i].Heading.values.tolist()\n",
    "      col=[\"skills\",\"Tech skills\",\"Projetcs\",\"Work_experience\",\"Language\"]#required columns\n",
    "      resume_skill=[]\n",
    "      for e in Entities:\n",
    "        if e in col:\n",
    "      \n",
    "          resume_skill.append(str(final_database[i][final_database[i][\"Heading\"]==e][\"Content\"].values))\n",
    "      info.append(''.join(resume_skill))\n",
    "        \n",
    "    return info  \n",
    "        \n",
    "      \n",
    "  def job_parser(self):\n",
    "      data = resumeparse.read_file(self.job)\n",
    "      job_skill=data[\"skills\"]\n",
    "      job_skill=' '.join(job_skill)\n",
    "\n",
    "      return job_skill\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  def cleaning_cv(self,docs):\n",
    "    cleaned=[]\n",
    "\n",
    "    #for resume\n",
    "    for doc in docs:\n",
    "        stop_words = list(set(stopwords.words('english')))\n",
    "        # split description into words with symbols attached + lower case\n",
    "        tokenized = word_tokenize(doc.lower())\n",
    "        #remove stop words\n",
    "        stopped=[w for w in tokenized if not w in stop_words]\n",
    "        \n",
    "        # handle special characters\n",
    "        cleaned.append(re.sub(r'[^(a-zA-Z)\\s]','',str(stopped)))\n",
    "    return cleaned  \n",
    "\n",
    "  def cleaning_job(self,doc):\n",
    "     #for JD   \n",
    "    cleaned=[]\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "      # split description into words with symbols attached + lower case\n",
    "    tokenized = word_tokenize(doc.lower())\n",
    "    #remove stop words\n",
    "    stopped=[w for w in tokenized if not w in stop_words]\n",
    "    # handle special characters\n",
    "    cleaned.append(re.sub(r'[^(a-zA-Z)\\s]','',str(stopped)))\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "8kbK3hEK93_p"
   },
   "outputs": [],
   "source": [
    "res=Resume(\"/content/Resume_txt\",\"/content/JD-vDoIT-React_Developer-ggn.pdf\")\n",
    "docs=res.resume_parser()\n",
    "job=res.job_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "f27qT6-YE8R_"
   },
   "outputs": [],
   "source": [
    "cleaned_docs=res.cleaning_cv(docs)\n",
    "cleaned_jobs=res.cleaning_job(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "NJUE3HwI8mBp"
   },
   "outputs": [],
   "source": [
    "train_set=cleaned_jobs+cleaned_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "ARXNu5LsMNK1"
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(train_set)]\n",
    "max_epochs = 100\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "\n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "R-zbWjQ_NdwY"
   },
   "outputs": [],
   "source": [
    "from gensim.models import doc2vec\n",
    "from scipy import spatial\n",
    "\n",
    "d2v_model = doc2vec.Doc2Vec.load(\"/content/d2v.model\")\n",
    "\n",
    "cos_distance=[]\n",
    "for i in range(len(cleaned_docs)):\n",
    "    vec1 = d2v_model.infer_vector(str(cleaned_docs[i]).split())\n",
    "    vec2 = d2v_model.infer_vector(str(cleaned_jobs).split())\n",
    "\n",
    "    cos_distance.append(spatial.distance.cosine(vec1, vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "ayUdmljqSUFh"
   },
   "outputs": [],
   "source": [
    "resume_match=pd.DataFrame(\n",
    "            {'Resume': file_name,\n",
    "            'Score': cos_distance})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "id": "mOy71WfFZ8Wp",
    "outputId": "a9a63e18-0fac-4197-8a09-4a65e4080869"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Resume</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Bijender_Singh_Resume.txt</td>\n",
       "      <td>0.262281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Ankit_Gupta_Resume.txt</td>\n",
       "      <td>0.158639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Aditya_Kumar_Resume.txt</td>\n",
       "      <td>0.147347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Shivam_Bisht_Resume.txt</td>\n",
       "      <td>0.124969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Piyush_Sharma_Resume.txt</td>\n",
       "      <td>0.119268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Abhishek_Bijalwan_Resume.txt</td>\n",
       "      <td>0.113368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jai_Mehta_Resume.txt</td>\n",
       "      <td>0.109532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Naman_Gupta_Resume.txt</td>\n",
       "      <td>0.096847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Purvansh_Arora_Resume.txt</td>\n",
       "      <td>0.091825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Aarushi_Dua_Resume.txt</td>\n",
       "      <td>0.090221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ritesh_Suryavanshi_Resume.txt</td>\n",
       "      <td>0.089439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Manish_Sengar_Resume.txt</td>\n",
       "      <td>0.084152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Himanshu_Resume.txt</td>\n",
       "      <td>0.082281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Rajat-Kumar-Arya-Resume.txt</td>\n",
       "      <td>0.080162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Syed_Adeeb_Resume.txt</td>\n",
       "      <td>0.079933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Ashish_Kumar_Resume.txt</td>\n",
       "      <td>0.078682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Abhishek_Jindal_Resume.txt</td>\n",
       "      <td>0.070604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mauktik_Madan_Resume.txt</td>\n",
       "      <td>0.069737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Chaitanya_Malik_Resume.txt</td>\n",
       "      <td>0.048523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Anup_Panwar_Resume.txt</td>\n",
       "      <td>0.041783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ankit_Kumar_Resume.txt</td>\n",
       "      <td>0.030263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Resume     Score\n",
       "14  Bijender_Singh_Resume.txt      0.262281\n",
       "20  Ankit_Gupta_Resume.txt         0.158639\n",
       "11  Aditya_Kumar_Resume.txt        0.147347\n",
       "13  Shivam_Bisht_Resume.txt        0.124969\n",
       "6   Piyush_Sharma_Resume.txt       0.119268\n",
       "10  Abhishek_Bijalwan_Resume.txt   0.113368\n",
       "3   Jai_Mehta_Resume.txt           0.109532\n",
       "8   Naman_Gupta_Resume.txt         0.096847\n",
       "5   Purvansh_Arora_Resume.txt      0.091825\n",
       "19  Aarushi_Dua_Resume.txt         0.090221\n",
       "0   Ritesh_Suryavanshi_Resume.txt  0.089439\n",
       "1   Manish_Sengar_Resume.txt       0.084152\n",
       "18  Himanshu_Resume.txt            0.082281\n",
       "16  Rajat-Kumar-Arya-Resume.txt    0.080162\n",
       "9   Syed_Adeeb_Resume.txt          0.079933\n",
       "12  Ashish_Kumar_Resume.txt        0.078682\n",
       "15  Abhishek_Jindal_Resume.txt     0.070604\n",
       "2   Mauktik_Madan_Resume.txt       0.069737\n",
       "7   Chaitanya_Malik_Resume.txt     0.048523\n",
       "17  Anup_Panwar_Resume.txt         0.041783\n",
       "4   Ankit_Kumar_Resume.txt         0.030263"
      ]
     },
     "execution_count": 149,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_match.sort_values(by=['Score'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "C7t9UQugOBFc"
   },
   "outputs": [],
   "source": [
    "\n",
    "def resume(mypath):\n",
    "    filename = mypath.replace(\"/content/Resume_pdf/\",\"\")\n",
    "\n",
    "    def_matching = resume_match[resume_match[\"Resume\"]==filename.replace('.pdf','.txt')]\n",
    "    output = {'filename': filename, 'matching%': def_matching}\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MHQ5jOeNa1re",
    "outputId": "64b74385-3bb7-4622-85db-2c2f3eaa88eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filename': 'Aarushi_Dua_Resume.pdf',\n",
       " 'matching%':                     Resume     Score\n",
       " 19  Aarushi_Dua_Resume.txt  0.090221}"
      ]
     },
     "execution_count": 151,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume('/content/Resume_pdf/Aarushi_Dua_Resume.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vX2jkGqkbFji"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": " Resume Notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
